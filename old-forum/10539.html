<html><header><title>How to improve data access performance for large object graph?</title></header><body><div style='float:right'><a href='http://cslanet.com'><img src='https://github.com/MarimerLLC/csla/raw/master/Support/Logos/csla%20win8_compact_s.png'/></a></div><p><h1>How to improve data access performance for large object graph?</h1><span style='font-size:small'>Old forum URL: forums.lhotka.net/forums/t/10539.aspx</span></p><hr><div style='padding:0 15 3 15;background-color:powderblue'><h2>Michael posted on Wednesday, July 20, 2011</h2><p>Hi everyone</p>
<p>We are trying to improve our application&#39;s performance when accessing a database over a VPN (Linq to SQL). It is not uncommon for a save to involve several hundred child BO&#39;s each calling SubmitChanges() within a single transaction. Over a VPN, this can take more than 60 seconds, compared to one or two seconds over a LAN.</p>
<p>These are large object graphs, with thousands of child objects. A real-world BO I&#39;m testing from our live database has a serialised size of 43MB, so an application server will not help in this case.</p>
<p>At this stage I&#39;ve had to increase the transaction timeout to three minutes because the saves were regularly timing out. Is there anything I can do to improve the performance?</p>
<p>Kind regards<br />Michael</p></div><div style='padding:0 15 3 15;background-color:powderblue'><h2>JonnyBee replied on Wednesday, July 20, 2011</h2><p>Why would you have each child BO call SubmitChanges?</p>
<p>Every time SubmitChanges is called your app will generate SQL and tak to the DB. </p>
<p>You can &quot;accumulate&quot; a lot of changes and call SubmitChanges just one time.</p></div><div style='padding:0 15 3 15;background-color:powderblue'><h2>Michael replied on Wednesday, July 20, 2011</h2><p>Hi Jonny</p>
<p>Before posting the question, I had the same thought, so I set up a test and changed the data access to only call SubmitChanges once, instead of 400 times. It made no difference at all: Both saves took 26 seconds over the slow VPN. So, I concluded that each row insert/update was still being fired off independently inside SubmitChanges, effectively making no difference.</p>
<p>Kind regards<br />Michael</p></div><div style='padding:0 15 3 15;background-color:powderblue'><h2>JonnyBee replied on Wednesday, July 20, 2011</h2><p>Did you run a memory profiler and database/sqlserver profiler as well to see which functions/code lines/sql&#39;s that were using time? </p>
<p>The only other option I can think of is to create a DiffGram of some kind (just the changed data) and send over to a server process (webservice or remote data portal) for updates to the database. </p>
<p>Or perhaps someone here has other suggestions?</p>
<p>&nbsp;</p></div><div style='padding:0 15 3 15;background-color:powderblue'><h2>Michael replied on Wednesday, July 20, 2011</h2><p>I didn&#39;t run any profilers. I have thought about sending only the data which has been changed instead of all the fields in each row. However, the aforementioned test was only doing inserts.</p></div><div style='padding:0 15 3 15;background-color:powderblue'><h2>RockfordLhotka replied on Thursday, July 21, 2011</h2><p>So you are running the DAL code such that the interaction with the database goes over the VPN?</p>
<p>I would recommend that you consider putting an app server in the same physical data center as the database server, and use the data portal to cross the VPN instead.</p>
<p>Generally, database interactions are very chatty (as you note in one of your posts). This means high latency network scenarios like a VPN make this type of interaction very slow.</p>
<p>The data portal interactions are usually not chatty - instead, they are chunky. One larger chunk of data is sent through, instead of lots of smaller chunks. High latency network scenarios have almost no impact on chunky communication.</p></div><div style='padding:0 15 3 15;background-color:powderblue'><h2>Michael replied on Thursday, July 21, 2011</h2><p>I had thought about an app server, but doesn&#39;t the entire business object need to be serialised and sent to to the app server? Our object graphs are very large, with serialised sizes in the tens of MB.</p>
<p>Our application is an AutoCAD add-on, and our BO saves are tied to the DWG document&#39;s save event (what&#39;s in the DWG and database should match). So our &quot;model&quot; object can have up to 1000 &quot;duct&quot; records, each with child collections as well, so it could easily be 5000+ objects. Most of the time there would only be a few dozen objects which have been touched between saves (if the operator is saving frequently), but sometimes it can be several hundred.</p>
<p>I think my next experiment will be to only send update the <i>fields</i> in the row which have been touched. This obviously won&#39;t help with new records, but hopefully it helps reduce the overall save times.</p></div><div style='padding:0 15 3 15;background-color:powderblue'><h2>RockfordLhotka replied on Thursday, July 21, 2011</h2><p>I see what you are saying. The core issue remains consistent though - chatty interaction over a VPN (or other high latency network) is problematic. You need some technique that allows a single call to cross the high latency network, so some processing can occur in the data center so the chatty database interaction occurs on a low latency network.</p></div><p style='font-size:small'>Copyright (c) Marimer LLC</body></html>
